const axios = require('axios');
const cheerio = require('cheerio');
const fs = require('fs');
const path = require('path');

const url = 'PAGE_URL';

const scrapePage = async () => {
  try {
    const { data } = await axios.get(url);
    const $ = cheerio.load(data);

    const results = $('div.grid.grid-cols-1 article').map((_, article) => {
      const $article = $(article);
      const linkTag = $article.find('a[title][href]');
      const postImgTag = $article.find('img[type="post"][srcset]');

      return {
        title: linkTag.attr('title') || null,
        url: linkTag.attr('href') || null,
        image: {
          src: postImgTag.attr('src') || null,
          srcset: postImgTag.attr('srcset') || null,
        },
      };
    }).get();

    const outputPath = path.join(__dirname, 'posts.json');
    fs.writeFileSync(outputPath, JSON.stringify(results, null, 2));
    console.log(`Scraped data saved to ${outputPath}`);
  } catch (error) {
    console.error('Error fetching the page:', error.message);
  }
};

scrapePage();
